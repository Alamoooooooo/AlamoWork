{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c395e39-0851-458e-92e8-5ecf33bb4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "from model import AE_MLP\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6587fb87-aed7-4ba2-bc9c-195300cac728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.FULLTRAIN = False\n",
    "        self.TEST = True\n",
    "        self.usegpu = True\n",
    "        self.purgedCV = True\n",
    "        self.gpuid = 0\n",
    "        self.seed = 42\n",
    "        self.model = 'AE_MLP'\n",
    "        self.use_wandb = False\n",
    "        self.use_tb = True\n",
    "        self.project = 'AE_MLP-purgedcv-with-lags-test'\n",
    "        self.save_model_root = f\"/root/autodl-tmp/JaneStreeReal2024/xdzy/models/{self.project}\"\n",
    "        os.makedirs(self.save_model_root, exist_ok=True) \n",
    "        self.dname = \"./input_df/\"\n",
    "        self.tbroot = \"/root/tf-logs\"\n",
    "        self.loader_workers = 6\n",
    "\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dropouts = [0.03527936123679956, 0.038424974585075086, 0.42409238408801436, 0.10431484318345882, 0.49230389137187497, 0.32024444956111164, 0.2716856145683449, 0.4379233941604448]\n",
    "        #self.dropouts = [0.035, 0.035, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "        if self.TEST:\n",
    "            self.bs = 8192  \n",
    "            self.lr =   1e-3\n",
    "            self.n_hidden = [96, 96, 896, 448, 448, 256]\n",
    "            self.patience = 5\n",
    "            self.max_epochs = 1\n",
    "            self.N_fold = 2\n",
    "        else:\n",
    "            self.bs = 4 * 8192\n",
    "            self.lr = 1e-3\n",
    "            self.n_hidden = [96, 96, 896, 448, 448, 256]\n",
    "            self.patience = 15\n",
    "            self.max_epochs = 2000\n",
    "            self.N_fold = 5\n",
    "        self.input_path = '../'\n",
    "        self.train_path = f\"{self.input_path}/training_data.parquet\"\n",
    "        self.valid_path = f\"{self.input_path}/validation_data.parquet\"\n",
    "        \n",
    "        self.feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "        self.label_name = 'responder_6'\n",
    "        self.weight_name = 'weight'\n",
    "        \n",
    "        self.test_train_ratio = 5\n",
    "        self.time_col = \"date_id\"  \n",
    "        self.group_gap = 31\n",
    "\n",
    "args = custom_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb7404e-b40c-4fb2-8c1a-dedf4577996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule)\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"4\"\n",
    "import polars as pol\n",
    " \n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            #train_array = []\n",
    "            #test_array = []\n",
    "            train_indices = set()  # 使用集合代替列表\n",
    "            test_indices = set()   # 使用集合代替列表\n",
    "            \n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                #train_array_tmp = group_dict[train_group_idx]\n",
    "                #train_array = np.sort(np.unique(np.concatenate((train_array,train_array_tmp)),axis=None), axis=None)\n",
    "            #train_end = train_array.size\n",
    "                train_indices.update(group_dict[train_group_idx])  # 使用 update 添加数据并自动去重\n",
    " \n",
    "            \n",
    "            \n",
    "            for test_group_idx in unique_groups[group_test_start:group_test_start +group_test_size]:\n",
    "                #test_array_tmp = group_dict[test_group_idx]\n",
    "                #test_array = np.sort(np.unique(np.concatenate((test_array,test_array_tmp)),axis=None), axis=None)\n",
    "            #test_array  = test_array[group_gap:]\n",
    "                test_indices.update(group_dict[test_group_idx])  # 使用 update 添加数据并自动去重\n",
    "            \n",
    "            #Optionally remove the group_gap from the test indices\n",
    "            #if self.group_gap:\n",
    "                #test_indices.difference_update(set(range(max(train_indices) + 1, max(train_indices) + 1 + self.group_gap)))\n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            #yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            yield sorted(train_indices), sorted(test_indices) \n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df,   accelerator):\n",
    "        self.features = torch.FloatTensor(df[args.feature_names].values).to(accelerator)\n",
    "        self.labels = torch.FloatTensor(df[args.label_name].values).to(accelerator)\n",
    "        self.weights = torch.FloatTensor(df[args.weight_name].values).to(accelerator)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_path, valid_path, batch_size, valid_df=None, time_col = \"date_id\", accelerator='cpu', TEST = False):\n",
    "        super().__init__()\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.dates = pol.scan_parquet(train_path).select(time_col).unique().collect().to_numpy().squeeze(-1).tolist()\n",
    "        self.batch_size = batch_size\n",
    "        self.accelerator = accelerator\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.TEST = TEST\n",
    "        \n",
    "\n",
    "    def setup(self,  train_selected_dates = None, valid_selected_dates = None, time_col = \"date_id\"):\n",
    "        \n",
    "        train = pol.scan_parquet(self.train_path).fill_null(strategy=\"forward\").fill_null(0)\n",
    "        valid = pol.scan_parquet(self.valid_path).fill_null(strategy=\"forward\").fill_null(0)\n",
    "        #train = pol.scan_parquet(f\"{input_path}/training_data.parquet\")\n",
    "        #valid = pol.scan_parquet(f\"{input_path}/validation_data.parquet\")\n",
    "\n",
    "        if train_selected_dates:\n",
    "            train = train.filter(pol.col(time_col).is_in(train_selected_dates))\n",
    "        if valid_selected_dates:\n",
    "            valid = valid.filter(pol.col(time_col).is_in(valid_selected_dates))\n",
    "\n",
    "        if self.TEST:\n",
    "            train = train.head(1000000)\n",
    "            valid = valid.head(100000)\n",
    "\n",
    "        \n",
    "        train = train.collect().to_pandas()\n",
    "        valid = valid.collect().to_pandas()\n",
    "\n",
    "        \n",
    "        print(train.shape, valid.shape )\n",
    "\n",
    "        self.train_dataset = CustomDataset(train, self.accelerator)\n",
    "    \n",
    "        self.val_dataset = CustomDataset(valid, self.accelerator)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def get_purged_cv_and_plot(self, N_fold, test_train_ratio, group_gap , time_col, save_figure=True):\n",
    "        \"\"\"\n",
    "        根据给定的时间列，生成PurgedGroupTimeSeriesSplit的CV划分并绘制图表。\n",
    "        \n",
    "        Args:\n",
    "         N_fold, test_train_ratio, group_gap ,\n",
    "            time_col: 时间列的名称。\n",
    "            save_figure: 是否保存图表。\n",
    "        \"\"\"\n",
    "        # 根据测试模式选择数据集\n",
    "        if self.TEST:\n",
    "            train = (\n",
    "                pol.scan_parquet(self.train_path)\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(0)\n",
    "                .filter(pol.col(time_col) >= 1500)\n",
    "                .collect()\n",
    "            )\n",
    "        else:\n",
    "            train = (\n",
    "                pol.scan_parquet(self.train_path)\n",
    "                .fill_null(strategy=\"forward\")\n",
    "                .fill_null(0)\n",
    "                .collect()\n",
    "            )\n",
    "        \n",
    "        # 数据处理\n",
    "        train = train.sort(time_col).to_pandas()\n",
    "        len_timeids = len(train[time_col].unique())\n",
    "        print(f\"总共天数: {len_timeids}\")\n",
    "        \n",
    "        # 计算划分参数\n",
    "        max_test_group_size = int(len_timeids / (N_fold + test_train_ratio))\n",
    "        max_train_group_size = max_test_group_size * test_train_ratio\n",
    "        print(f\"max_test_group_size: {max_test_group_size}\")\n",
    "        print(f\"max_train_group_size: {max_train_group_size}\")\n",
    "        print(f\"group_gap: {group_gap}\")\n",
    "        \n",
    "        # 创建CV划分对象\n",
    "        cv = PurgedGroupTimeSeriesSplit(\n",
    "            n_splits=N_fold,\n",
    "            max_train_group_size=max_train_group_size,\n",
    "            group_gap=group_gap,\n",
    "            max_test_group_size=max_test_group_size,\n",
    "        )\n",
    "        \n",
    "        # 生成和可视化CV划分\n",
    "        splits = cv.split(train, groups=train[time_col].to_numpy())\n",
    "        plt.figure(figsize=(24, 8))\n",
    "        dates_each_fold = []\n",
    "        \n",
    "        for i, (train_idx, test_idx) in enumerate(splits):\n",
    "            print(f\"Processing fold {i}\")\n",
    "            \n",
    "            # 获取训练和测试时间范围\n",
    "            train_start_date = train[time_col][train_idx[0]]\n",
    "            train_end_date = train[time_col][train_idx[-1]]\n",
    "            test_start_date = train[time_col][test_idx[0]]\n",
    "            test_end_date = train[time_col][test_idx[-1]]\n",
    "            \n",
    "            # 记录日期范围\n",
    "            train_dates = range(train_start_date, train_end_date + 1)\n",
    "            test_dates = range(test_start_date, test_end_date + 1)\n",
    "            dates_each_fold.append((train_dates, test_dates))\n",
    "            \n",
    "            # 打印范围信息\n",
    "            print(f\"fold {i} train: {train_start_date} - {train_end_date}\")\n",
    "            print(f\"fold {i} test: {test_start_date} - {test_end_date}\")\n",
    "            \n",
    "            # 绘制训练数据\n",
    "            plt.plot([train_start_date, train_end_date], [i, i], marker='o', color='blue')\n",
    "            plt.text(train_start_date, i + 0.1, f\"{train_start_date}\", ha='right')\n",
    "            plt.text(train_end_date, i + 0.1, f\"{train_end_date}\", ha='right')\n",
    "            \n",
    "            # 绘制测试数据\n",
    "            plt.plot([test_start_date, test_end_date], [i, i], marker='o', color='red')\n",
    "            plt.text(test_start_date, i - 0.2, f\"{test_start_date}\", ha='right')\n",
    "            plt.text(test_end_date, i - 0.2, f\"{test_end_date}\", ha='right')\n",
    "        \n",
    "        # 清理资源\n",
    "        del train\n",
    "        gc.collect()\n",
    "        \n",
    "        # 图表设置与保存\n",
    "        plt.title('Purged_Group_TimeSeries_Split')\n",
    "        plt.xlabel('Time_Col')\n",
    "        plt.ylabel('CV_Iteration')\n",
    "        if save_figure:\n",
    "            plt.savefig('./cv_plan.png')\n",
    "\n",
    "        return dates_each_fold\n",
    "\n",
    "    def setup_purged_cv(self, train_selected_dates, valid_selected_dates, time_col):\n",
    "        \n",
    "        df = pol.scan_parquet(self.train_path).fill_null(strategy=\"forward\").fill_null(0)\n",
    "        \n",
    "        train = df.filter(pol.col(time_col).is_in(train_selected_dates))\n",
    "        valid = df.filter(pol.col(time_col).is_in(valid_selected_dates))\n",
    "            \n",
    "        train = train.collect().to_pandas()\n",
    "        valid = valid.collect().to_pandas()\n",
    "        \n",
    "        print(train.shape, valid.shape)\n",
    "\n",
    "        self.train_dataset = CustomDataset(train, self.accelerator)\n",
    "\n",
    "        self.val_dataset = CustomDataset(valid, self.accelerator)\n",
    "\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def train_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)#, pin_memory=True, persistent_workers=True)#\n",
    "\n",
    "    def val_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)#, persistent_workers=True)#, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500742e-fdc4-47f7-acac-f6543001e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 检查设备\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "loader_device = 'cpu'\n",
    "\n",
    "if args.FULLTRAIN:\n",
    "    data_module = DataModule([args.train_path , args.valid_path ], args.valid_path, batch_size=args.bs, accelerator=loader_device, TEST = args.TEST)\n",
    "else:\n",
    "    data_module = DataModule(args.train_path, args.valid_path, batch_size=args.bs, accelerator=loader_device, TEST = args.TEST)\n",
    "\n",
    "dates_each_fold = data_module.get_purged_cv_and_plot(args.N_fold, args.test_train_ratio, args.group_gap , args.time_col )\n",
    "\n",
    "# 设置全局随机种子\n",
    "pl.seed_everything(args.seed)\n",
    "\n",
    "saved_epochs = {}\n",
    "\n",
    "for fold, (train_dates, valid_dates) in enumerate(dates_each_fold):\n",
    "    print(f\"setup date for fold {fold}\")\n",
    "    print(f\"fold {fold} train: {train_dates[0]} - {train_dates[-1]}\")\n",
    "    print(f\"fold {fold} test: {valid_dates[0]} - {valid_dates[-1]}\")\n",
    "    data_module.setup_purged_cv(train_dates, valid_dates, args.time_col)\n",
    "    \n",
    "    print(f\"success setup fold {fold}\")\n",
    "\n",
    "    # 获取输入维度\n",
    "    input_dim = data_module.train_dataset.features.shape[1]\n",
    "\n",
    "    # 初始化模型\n",
    "    model = AE_MLP(\n",
    "        num_columns=input_dim,\n",
    "        num_labels=1,\n",
    "        hidden_units=args.n_hidden,\n",
    "        dropout_rates=args.dropouts,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "\n",
    "    # 初始化日志记录器\n",
    "    if args.use_wandb:\n",
    "        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "        logger = WandbLogger(experiment=wandb_run)\n",
    "    elif args.use_tb:\n",
    "        logger = TensorBoardLogger(args.tbroot, name=f\"{args.project}_fold_{fold}\")\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    # 初始化回调函数\n",
    "    early_stopping = EarlyStopping('val_r_square', patience=args.patience, mode='max', verbose=False)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_r_square', mode='max', save_top_k=1, verbose=True, filename=os.path.join(args.save_model_root, f\"nn_{fold}.model\"))\n",
    "    timer = Timer()\n",
    "\n",
    "    # 初始化Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=[args.gpuid] if args.usegpu else None,\n",
    "        logger=logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n",
    "    \n",
    "    saved_epochs[fold] = trainer.train_epoch_record\n",
    "    print(f\"Fold-{fold} Training completed in {timer.time_elapsed('train'):.2f}s\")\n",
    "\n",
    "# 保存 saved_epochs 到 JSON 文件\n",
    "with open(os.path.join(args.save_model_root, f\"saved_epochs.json\") , \"w\") as f:\n",
    "    json.dump(saved_epochs, f, indent=4)\n",
    "print(\"Saved epochs information to 'saved_epochs.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b200716-e44c-441e-bd4d-4ea63b7ebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "    saved_epochs[fold] = {\n",
    "        'best_score': float(best_epoch) if best_epoch is not None else None,\n",
    "        'best_epoch': best_epoch_index\n",
    "    }\n",
    "\n",
    "    print(f\"Fold-{fold} Training completed in {timer.time_elapsed('train'):.2f}s\")\n",
    "    print(f\"Best model saved for fold {fold} at epoch {best_epoch_index} with score {best_epoch}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c591a-cbc1-4b43-b931-1645cb620ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后用最后一个fold最好的epoch训练一个日期最新的模型，模型训练集长度和之前一样\n",
    "# 打印所有保存的epoch信息\n",
    "print(\"Saved model epochs information:\")\n",
    "for fold, info in saved_epochs.items():\n",
    "    print(f\"Fold {fold}: Best Epoch = {info['best_epoch']}, Best Score = {info['best_score']}\")\n",
    "\n",
    "# 保存 saved_epochs 到 JSON 文件\n",
    "with open(os.path.join(args.save_model_root, f\"saved_epochs.json\") , \"w\") as f:\n",
    "    json.dump(saved_epochs, f, indent=4)\n",
    "print(\"Saved epochs information to 'saved_epochs.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78061414-9e9c-49ff-b960-8948cc0f17fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
